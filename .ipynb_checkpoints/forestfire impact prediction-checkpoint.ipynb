{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://upload.wikimedia.org/wikipedia/commons/d/d8/Deerfire_high_res_edit.jpg' width='1200px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Topics-covered\" data-toc-modified-id=\"Topics-covered-1\">Topics covered</a></span></li><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-2\">Objective</a></span></li><li><span><a href=\"#Define-the-metrics\" data-toc-modified-id=\"Define-the-metrics-3\">Define the metrics</a></span></li><li><span><a href=\"#Dependencies\" data-toc-modified-id=\"Dependencies-4\">Dependencies</a></span></li><li><span><a href=\"#Load-and-describe-data\" data-toc-modified-id=\"Load-and-describe-data-5\">Load and describe data</a></span></li><li><span><a href=\"#Missing-value-treatment\" data-toc-modified-id=\"Missing-value-treatment-6\">Missing value treatment</a></span></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-7\">Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Univariate-analysis\" data-toc-modified-id=\"Univariate-analysis-7.1\">Univariate analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Let's-begin-with-the-target-variable,-Area\" data-toc-modified-id=\"Let's-begin-with-the-target-variable,-Area-7.1.1\">Let's begin with the target variable, <code>Area</code></a></span></li><li><span><a href=\"#Independent-columns\" data-toc-modified-id=\"Independent-columns-7.1.2\">Independent columns</a></span></li><li><span><a href=\"#Categorical-columns\" data-toc-modified-id=\"Categorical-columns-7.1.3\">Categorical columns</a></span></li><li><span><a href=\"#Numerical-Columns\" data-toc-modified-id=\"Numerical-Columns-7.1.4\">Numerical Columns</a></span></li></ul></li><li><span><a href=\"#Bivariate-analysis-with-our-target-variable\" data-toc-modified-id=\"Bivariate-analysis-with-our-target-variable-7.2\">Bivariate analysis with our target variable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-columns\" data-toc-modified-id=\"Categorical-columns-7.2.1\">Categorical columns</a></span></li><li><span><a href=\"#Numerical-columns\" data-toc-modified-id=\"Numerical-columns-7.2.2\">Numerical columns</a></span></li></ul></li><li><span><a href=\"#Multivariate-analysis\" data-toc-modified-id=\"Multivariate-analysis-7.3\">Multivariate analysis</a></span></li></ul></li><li><span><a href=\"#Outlier-treatment\" data-toc-modified-id=\"Outlier-treatment-8\">Outlier treatment</a></span></li><li><span><a href=\"#Preparing-the-data-for-modelling\" data-toc-modified-id=\"Preparing-the-data-for-modelling-9\">Preparing the data for modelling</a></span></li><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-10\">Linear Regression</a></span></li><li><span><a href=\"#Statistical-approach\" data-toc-modified-id=\"Statistical-approach-11\">Statistical approach</a></span><ul class=\"toc-item\"><li><span><a href=\"#Checking-assumptions-for-linear-regression-in-statistics\" data-toc-modified-id=\"Checking-assumptions-for-linear-regression-in-statistics-11.1\">Checking assumptions for linear regression in statistics</a></span></li><li><span><a href=\"#1.-Linearity-of-residuals\" data-toc-modified-id=\"1.-Linearity-of-residuals-11.2\">1. Linearity of residuals</a></span></li><li><span><a href=\"#2.-Normality-of-the-residuals\" data-toc-modified-id=\"2.-Normality-of-the-residuals-11.3\">2. Normality of the residuals</a></span></li><li><span><a href=\"#3.-Homoscedasticity\" data-toc-modified-id=\"3.-Homoscedasticity-11.4\">3. Homoscedasticity</a></span></li><li><span><a href=\"#4.-No-Autocorrelation\" data-toc-modified-id=\"4.-No-Autocorrelation-11.5\">4. No Autocorrelation</a></span></li><li><span><a href=\"#5.-Multicollinearity\" data-toc-modified-id=\"5.-Multicollinearity-11.6\">5. Multicollinearity</a></span></li></ul></li><li><span><a href=\"#Machine-learning-approach\" data-toc-modified-id=\"Machine-learning-approach-12\">Machine learning approach</a></span></li><li><span><a href=\"#Improving-Stats-model\" data-toc-modified-id=\"Improving-Stats-model-13\">Improving Stats model</a></span></li><li><span><a href=\"#Improving-ML-model\" data-toc-modified-id=\"Improving-ML-model-14\">Improving ML model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective \n",
    "\n",
    "Forest fires help in the natural cycle of woods' growth and replenishment. They Clear dead trees, leaves, and competing vegetation from the forest floor, so new plants can grow. Remove weak or disease-ridden trees, leaving more space and nutrients for stronger trees.\n",
    "\n",
    "\n",
    "But when fires burn too hot and uncontrollable or when they’re in the “wildland-urban interface” (the places where woodlands and homes or other developed areas meet), they can be damaging and life threatning.\n",
    "\n",
    "\n",
    "In this kernel, our aim is to predict the burned area (`area`) of forest fires, in the northeast region of Portugal. Based on the the spatial, temporal, and weather variables where the fire is spotted. \n",
    "\n",
    "This prediction can be used for calculating the forces sent to the incident and deciding the urgency of the situation.\n",
    "\n",
    "Further read:\n",
    "1. [Mylandplan](https://mylandplan.org/content/good-and-bad-forest-fires)\n",
    "2. [KNIME](https://www.knime.com/knime-applications/forest-fire-prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'area'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSE** \n",
    "\n",
    "RMSE is the most popular evaluation metric used in regression problems. It follows an assumption that error are unbiased and follow a normal distribution.\n",
    "\n",
    "Further read: https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import zscore\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'forestfires.csv'\n",
    "path = \"../input/forest-fires-data-set/forestfires.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "   We will try out the following analysis on our dataset\n",
    "   - Univariate \n",
    "   - Bivariate \n",
    "   - Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 9,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's begin with the target variable, `Area`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "print(\"Skew: {}\".format(df[target].skew()))\n",
    "print(\"Kurtosis: {}\".format(df[target].kurtosis()))\n",
    "ax = sns.kdeplot(df[target],shade=True,color='g')\n",
    "plt.xticks([i for i in range(0,1200,50)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(df[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Few observations:**\n",
    "\n",
    "- The data is highly skewed with a value of +12.84 and huge kurtosis value of 194.\n",
    "\n",
    "- It even tells you that majority of the forest fires do not cover a large area, most of the damaged area is under 50 hectares of land.\n",
    "\n",
    "- We can apply tranformation to fix the skewnesss and kurtosis, however we will have to inverse transform before submitting the output.\n",
    "\n",
    "- Outlier Check: There are 4 outlier instances in our area columns but the questions is should we drop it or not? (Will get back to this in the outlier treatment step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier points\n",
    "y_outliers = df[abs(zscore(df[target])) >= 3 ]\n",
    "y_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = df.drop(columns=target)\n",
    "cat_columns = dfa.select_dtypes(include='object').columns.tolist()\n",
    "num_columns = dfa.select_dtypes(exclude='object').columns.tolist()\n",
    "\n",
    "cat_columns,num_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing categorical columns\n",
    "plt.figure(figsize=(16,10))\n",
    "for i,col in enumerate(cat_columns,1):\n",
    "    plt.subplot(2,2,i)\n",
    "    sns.countplot(data=dfa,y=col)\n",
    "    plt.subplot(2,2,i+2)\n",
    "    df[col].value_counts(normalize=True).plot.bar()\n",
    "    plt.ylabel(col)\n",
    "    plt.xlabel('% distribution per category')\n",
    "plt.tight_layout()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It is interesting to see that abnormally high number of the forest fires occur in the month of `August`\n",
    "and `September`.\n",
    "\n",
    "2. In the case of day, the days `Friday` to `Monday` have higher proportion of cases. (However, no strong indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,40))\n",
    "for i,col in enumerate(num_columns,1):\n",
    "    plt.subplot(8,4,i)\n",
    "    sns.kdeplot(df[col],color='g',shade=True)\n",
    "    plt.subplot(8,4,i+10)\n",
    "    df[col].plot.box()\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "num_data = df[num_columns]\n",
    "pd.DataFrame(data=[num_data.skew(),num_data.kurtosis()],index=['skewness','kurtosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers, Skewness and kurtosis (high positive or negative) was observed in the following columns:\n",
    "1. FFMC\n",
    "2. ISI\n",
    "3. rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate analysis with our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['area'].describe(),'\\n')\n",
    "print(y_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a categorical variable based on forest fire area damage\n",
    "# No damage, low, moderate, high, very high\n",
    "def area_cat(area):\n",
    "    if area == 0.0:\n",
    "        return \"No damage\"\n",
    "    elif area <= 1:\n",
    "        return \"low\"\n",
    "    elif area <= 25:\n",
    "        return \"moderate\"\n",
    "    elif area <= 100:\n",
    "        return \"high\"\n",
    "    else:\n",
    "        return \"very high\"\n",
    "\n",
    "df['damage_category'] = df['area'].apply(area_cat)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in cat_columns:\n",
    "    cross = pd.crosstab(index=df['damage_category'],columns=df[col],normalize='index')\n",
    "    cross.plot.barh(stacked=True,rot=40,cmap='hot')\n",
    "    plt.xlabel('% distribution per category')\n",
    "    plt.xticks(np.arange(0,1.1,0.1))\n",
    "    plt.title(\"Forestfire damage each {}\".format(col))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Previously we had observed that `August` and `September` had the most number of forest fires. And from the above plot of `month`, we can understand few things\n",
    "    - Most of the fires in August were low (< 1 hectare).\n",
    "    - The very high damages(>100 hectares) happened in only 3 months - august,july and september.\n",
    " \n",
    "- Regarding fire damage per day, nothing much can be observed. Except that, there were no ` very high` damaging fires on Friday and on Saturdays it has been reported most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,40))\n",
    "for i,col in enumerate(num_columns,1):\n",
    "    plt.subplot(10,1,i)\n",
    "    if col in ['X','Y']:\n",
    "        sns.swarmplot(data=df,x=col,y=target,hue='damage_category')\n",
    "    else:\n",
    "        sns.scatterplot(data=df,x=col,y=target,hue='damage_category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = df.drop(columns=['damage_category','day','month']).columns\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,hue='damage_category',vars=selected_features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had observed outliers in the following columns:\n",
    "1. area \n",
    "2. FFMC\n",
    "2. ISI\n",
    "3. rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_columns = ['area','FFMC','ISI','rain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the above outliers are not error values so we cannot remove it. \n",
    "\n",
    "In order to minimize the effect of outliers in our model we will transform the above features. \n",
    "\n",
    "**Ref:** https://humansofdata.atlan.com/2018/03/when-delete-outliers-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for modelling\n",
    "Thing which we can cover here\n",
    "- Encoding the categorical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df,columns=['day','month'],drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data transformations like `log,inverse,exponential`,etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[out_columns].describe())\n",
    "np.log1p(df[out_columns]).skew(), np.log1p(df[out_columns]).kurtosis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFMC and rain are still having high skew and kurtosis values, \n",
    "# since we will be using Linear regression model we cannot operate with such high values\n",
    "# so for FFMC we can remove the outliers in them using z-score method\n",
    "mask = df.loc[:,['FFMC']].apply(zscore).abs() < 3\n",
    "\n",
    "# Since most of the values in rain are 0.0, we can convert it as a categorical column\n",
    "df['rain'] = df['rain'].apply(lambda x: int(x > 0.0))\n",
    "\n",
    "df = df[mask.values]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_columns.remove('rain')\n",
    "df[out_columns] = np.log1p(df[out_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[out_columns].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this kernel we will be working on Linear regression using both Statistical and Machine learning approach.**\n",
    "\n",
    "<img src=\"//i.imgflip.com/3hzd4f.jpg\"/>\n",
    "\n",
    "\n",
    "**Difference between statistical and machine learning approach**\n",
    "\n",
    "- Machine learning produces **predictions**.  As far as I can tell, it is not very good at drawing conclusions about general principles based on a set of observations.\n",
    "- Statistical estimation lets the practitioner make **inferences** (conclusions about a larger set of phenomena based on the observation of a smaller set of phenomena.)  For example, in a regression model the practitioner can estimate the effect of a one unit change in an independent variable X on a dependent variable y.\n",
    "\n",
    "Further read: [Quora](https://www.quora.com/When-do-you-use-machine-learning-vs-statistical-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['area','damage_category'])\n",
    "y = df['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking assumptions for linear regression in statistics\n",
    "\n",
    "1. Linearity of model\n",
    "    \n",
    "2. Normality of residuals\n",
    "\n",
    "3. Homoscedasticity\n",
    "\n",
    "4. No Autocorrelation\n",
    "\n",
    "5. Multicollinearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_constant = sm.add_constant(X)\n",
    "\n",
    "# Build OLS model\n",
    "lin_reg = sm.OLS(y,X_constant).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linearity of residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linearity can be measured by two methods:**\n",
    "\n",
    "- Plot the observed values Vs predicted values` and plot the `Residual Vs predicted values` and see the linearity of residuals. \n",
    "-  Rainbow test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearity_test(model, y):\n",
    "    '''\n",
    "    Function for visually inspecting the assumption of linearity in a linear regression model.\n",
    "    It plots observed vs. predicted values and residuals vs. predicted values.\n",
    "    \n",
    "    Args:\n",
    "    * model - fitted OLS model from statsmodels\n",
    "    * y - observed values\n",
    "    '''\n",
    "    fitted_vals = model.predict()\n",
    "    resids = model.resid\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n",
    "    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)\n",
    "    ax[0].set(xlabel='Predicted', ylabel='Observed')\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})\n",
    "    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)\n",
    "    ax[1].set(xlabel='Predicted', ylabel='Residuals')\n",
    "    \n",
    "linearity_test(lin_reg, y) \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The desired outcome is that points are symmetrically distributed around a diagonal line in the former plot or around horizontal line in the latter one. \n",
    "\n",
    "- By observing  the plots the linearity assumption is not there \n",
    "\n",
    "- Adding new features might result in linearity of model \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rainbow test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import pylab\n",
    "\n",
    "# get an instance of Influence with influence and outlier measures \n",
    "st_resid = lin_reg.get_influence().resid_studentized_internal\n",
    "stats.probplot(st_resid,dist=\"norm\",plot=pylab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expectation Mean of residual is zero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.resid.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The value is close to zero. So, linearity is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normality of the residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test for normality: Jarque Bera**\n",
    "\n",
    "For a good model, the residuals should be normally distributed.\n",
    "The higher the value of Jarque Bera test, the lesser the residuals are normally distributed.\n",
    "\n",
    "The Jarque–Bera test is a goodness-of-fit test of whether sample data \n",
    "have the skewness and kurtosis matching a normal distribution.\n",
    "\n",
    "> Jarque-Bera (JB):\t107.018\n",
    "\n",
    "The jarque bera test tests whether the sample data has the skewness and kurtosis matching a normal distribution.\n",
    "\n",
    "Note that this test generally works good for large enough number of data samples(>2000) as the test statistics asymptotically has a chi squared distribution with degrees 2 of freedom.\n",
    "\n",
    "> Our dataframe shape, 517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(lin_reg.resid,line ='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the normality of residuals by removing the outliers but in our problem we need the outliers because of its feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Homoscedasticity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQ78MxgdAcvdPa-45oY67MVwA0P3AVrVCtJO_6nYytQv9jJXjxF\" />\n",
    "\n",
    "Homoscedacity: If the residuals are symmetrically distributed across the trend , then it is called as homoscedacious. \n",
    "\n",
    "Heteroscedacity: If the residuals are not symmetric across the trend, then it is called as heteroscedacious.\n",
    "\n",
    "\n",
    "**Null and alternate hypothesis are:**\n",
    "\n",
    "H0 = constant variance among residuals (Homoscedacity)\n",
    "\n",
    "Ha = Heteroscedacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.compat import lzip\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "model = lin_reg\n",
    "fitted_vals = model.predict()\n",
    "resids = model.resid\n",
    "resids_standardized = model.get_influence().resid_studentized_internal\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n",
    "ax[0].set_title('Residuals vs Fitted', fontsize=16)\n",
    "ax[0].set(xlabel='Fitted Values', ylabel='Residuals')\n",
    "\n",
    "sns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'})\n",
    "ax[1].set_title('Scale-Location', fontsize=16)\n",
    "ax[1].set(xlabel='Fitted Values', ylabel='sqrt(abs(Residuals))')\n",
    "\n",
    "name = ['F statistic', 'p-value']\n",
    "test = sms.het_goldfeldquandt(model.resid, model.model.exog)\n",
    "lzip(name, test)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To identify homoscedasticity in the plots, the placement of the points should be random and no pattern (increase/decrease in values of residuals) should be visible and P-Values should be less than 0.05 \n",
    "- In the plots we can see there are paticular patterns and P-Values is also greater than 0.05 ,so we can say that there is no homoscedasticity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. No Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation measures the relationship between a variable's current value and its past values.\n",
    "\n",
    "**Test for autocorrelation : Durbin- Watson Test**\n",
    "\n",
    "It's value ranges from 0-4. If the value is between \n",
    "- 0-2, it's known as Positive Autocorrelation.\n",
    "- 2-4, it is known as Negative autocorrelation.\n",
    "- exactly 2, it means No Autocorrelation.\n",
    "\n",
    "For a good linear model, it should have low or no autocorrelation.\n",
    "\n",
    "\n",
    "> In our case, Durbin-Watson: 0.979"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "acf = smt.graphics.plot_acf(lin_reg.resid, lags=50 , alpha=0.05)\n",
    "acf.show()\n",
    "# Confidence intervals are drawn as a cone. \n",
    "# By default, this is set to a 95% confidence interval, \n",
    "# suggesting that correlation values outside of this code are very likely a correlation \n",
    "# and not a statistical fluke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By observing the above data we can say that there is positive autocorrelation is present , we can reduce it by using fine tuning our parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multicollinearity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollineariy arises when one independent variable can be linearly predicted by others with a substantial level of accuracy.\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcR7jSx3cImz0dmtPHejjtlJAU8MwhK0mjbZxc7Wvu_aE4PkCMYO\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(16,10))\n",
    "\n",
    "sns.heatmap(df.corr(),annot=True,cmap='YlGnBu',fmt=\".2f\",cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\n",
    "pd.DataFrame({'vif': vif[1:]}, index=X.columns).sort_values(by=\"vif\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there is multicollinearity present between some features where vif >5.\n",
    "- To deal with multicollinearity we should iteratively remove features with high values of VIF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Intercept: {lr.intercept_}')\n",
    "print(f'R^2 score: {lr.score(X, y)}')\n",
    "pd.DataFrame({\"Coefficients\": lr.coef_}, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Stats model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping columns to improve accuracy:**\n",
    "    \n",
    "By checking high Variance inflation factor and p-value we will decide whether to keep the column or drop it.\n",
    "\n",
    "> R^2 = 1 - SSE(Sum of Square of Residuals)/SST (Sum of square Total)\n",
    "\n",
    "Just by dropping constant we got a huge bump in adjusted R2 from `2.5%` to `40.6%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['area','damage_category'])\n",
    "y = df['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stats(X,y):\n",
    "    vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    print(pd.DataFrame({'vif': vif}, index=X.columns).sort_values(by=\"vif\",ascending=False)[:10])\n",
    "    lin_reg = sm.OLS(y,X).fit()\n",
    "    print(lin_reg.summary())\n",
    "check_stats(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['FFMC'],inplace=True)\n",
    "# check_stats(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['Y'],inplace=True)\n",
    "# check_stats(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['month_jul'],inplace=True)\n",
    "# check_stats(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['day_thu'],inplace=True)\n",
    "# check_stats(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['day_mon'],inplace=True)\n",
    "# check_stats(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['month_aug'],inplace=True)\n",
    "check_stats(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can continue to optimize the model.\n",
    "\n",
    "Our Prob (F-statistic) has improved from 0.0558 to 2.20e-48. As the value tends to 0, the model becomes more significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
